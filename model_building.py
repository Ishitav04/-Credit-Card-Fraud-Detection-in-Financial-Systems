# -*- coding: utf-8 -*-
"""model_building.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8q3k0Dkwp6OTTA_amfactV4R0wH4B6j

# DATA PREPARATION
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from imblearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score, f1_score
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve

data = pd.read_csv('/content/creditcard.csv')

data.head()

data.describe()

print("Missing Values:\n", data.isnull().sum().max())

data.columns

print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')
print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')



colors = ["#0101DF", "#DF0101"]
sns.countplot(x='Class', data=data, palette=colors)
plt.title('Class Distributions \n (0: No Fraud || 1: Fraud)', fontsize=14)
plt.show()

# Distribution of transaction amount
fig, ax = plt.subplots(1, 2, figsize=(18,4))
amount_val = data['Amount'].values
time_val = data['Time'].values
sns.distplot(amount_val, ax=ax[0], color='r')
ax[0].set_title('Distribution of Transaction Amount', fontsize=14)
ax[0].set_xlim([min(amount_val), max(amount_val)])
sns.distplot(time_val, ax=ax[1], color='b')
ax[1].set_title('Distribution of Transaction Time', fontsize=14)
ax[1].set_xlim([min(time_val), max(time_val)])
plt.show()

# Normalizing time and Amount
scaler = RobustScaler()
data['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

print(data.head())

"""# SMOTE"""

# Step 1: Split features and target
X = data.drop('Class', axis=1)
y = data['Class']

# Step 2: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Step 3: Apply SMOTE to the training set
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("Class distribution before SMOTE:")
print(y_train.value_counts())

print("\nClass distribution after SMOTE:")
print(y_train_smote.value_counts())

sns.countplot(x=y_train_smote)
plt.title("Class Distribution After SMOTE Oversampling")
plt.show()

# Correlation matrix and heatmap for imbalanced data
corr_matrix_original = data.corr()
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix_original, annot=False, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix (Imbalanced Data)")
plt.show()


# Correlation matrix and heatmap for SMOTE processed data
data_smote = pd.concat([X_train_smote, y_train_smote], axis=1)
corr_matrix_smote = data_smote.corr()
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix_smote, annot=False, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix (SMOTE Processed Data)")
plt.show()

"""# training the XGBoost Model"""

# Train XGBoost
model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

model.fit(X_train_smote, y_train_smote)

# Evaluate the model
y_pred = model.predict(X_test)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Plot feature importance
importances = model.feature_importances_
sns.barplot(x=importances, y=X.columns)
plt.title("Feature Importance")
plt.show()

y_pred_proba = model.predict_proba(X_test)[:, 1]

# Predict classes
y_pred = model.predict(X_test)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = roc_auc_score(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.grid()
plt.show()

"""# SAVING THE MODEL

"""

import joblib

joblib.dump(scaler, 'scaler.pkl')

joblib.dump(model, 'xgboost_model.pkl')

from google.colab import files
files.download('xgboost_model.pkl')

from google.colab import files
files.download('scaler.pkl')

import pickle

# Save the model
with open('xgboost_model.pkl', 'wb') as file:
    pickle.dump(model, file)

# Download the model
from google.colab import files
files.download('xgboost_model.pkl')